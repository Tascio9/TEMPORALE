-- 2021-02-18
Passo a W2V.
Inizio con l'original.
# CBOW (Continuous Bag of Words) [https://media.geeksforgeeks.org/wp-content/uploads/cbow-1.png]
word2vec = Word2Vec(dataset_w2v.values(), min_count=3, size=100, window=5) # size 100 by default
X = (19700, 100)
Elbow curve.
Cluster 5.
Silhouette Coefficient: 0.53445053

SVD a 75 features.
Silhouette Coefficient: 0.06819047

tSNE
Silhouette Coefficient: 0.360

-- 2021-02-18
Inizio a processare l'ultimo e definitvo dataset(cord-19_2021-02-15):
https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2021-02-15.tar.gz
Procedo con estrarrere abstract SOLO dei paper 2020-2021 e utilizzo LEMMATIZER.
TFIDF: vectorizer = TfidfVectorizer(stop_words='english', use_idf=True, 
                             smooth_idf=True, ngram_range=(1,2), min_df=100) ## Corpus is in English
Ottengo alla fine X = (66979, 5859) [paper x terms].
Cerco con Elbow curve il gomito -> Dataset210215_2021_abstract_lem_eng_10.
Data la curva, scelgo K=3. In 210215/Originals ottengo le 3 classificazioni.
Cluster 0: "health" "pandemic" "model" "data" "social" "care" "time" "number" "public" "analysis"
Cluster 1: "severe" "respiratory" "clinical" "infection" "mortality" "day" "risk" "acute" "syndrome" "treatment"
Cluster 2: "protein" "viral" "ace" "cell" "immune" "infection" "binding" "human" "host" "expression"
Silhouette Coefficient: 0.005495839093630324


Decido di tracciare la varianza -> Variance.png
Scelgo max_features = 500 ed disegno di nuovo l'Elbow curve -> Dataset210215_2021_abstract_lem_eng_10_500features
Di nuovo, scelgo K=3 e clusterizzo.
Cluster 0: "protein" "viral" "cell" "ace" "immune" "human" "infection" "binding" "host" "expression"
Cluster 1: "health" "pandemic" "model" "data" "social" "care" "time" "number" "public" "analysis"
Cluster 2: "clinical" "severe" "respiratory" "infection" "day" "mortality" "risk" "acute" "treatment" "patient"
Silhouette Coefficient: 0.013

tSNE 
Silhouette Coefficient: 0.382

Passo al trovare il PERFECT DATASET.
X diventa adesso (36438, 3643).
Traccio Elbow Curve -> Dataset210215_2021_abstract_lem_eng_10_perfect
Cluster 0: "health" "pandemic" "data" "model" "social" "care" "time" "public" "analysis" "number"
Cluster 1: "severe" "respiratory" "clinical" "infection" "day" "acute" "mortality" "risk" "syndrome" "treatment"
Cluster 2: "protein" "viral" "ace" "cell" "infection" "binding" "immune" "human" "host" "spike"
Silhouette Coefficient: 0.006620304587912687

SVD a 500 features. Cluster a 3 -> Dataset210215_2021_abstract_lem_eng_10_500features_perfect
Cluster 0: "protein" "viral" "cell" "ace" "infection" "human" "immune" "binding" "host" "antiviral"
Cluster 1: "health" "pandemic" "data" "model" "social" "care" "time" "analysis" "paper" "public"
Cluster 2: "clinical" "severe" "respiratory" "infection" "day" "acute" "mortality" "risk" "treatment" "syndrome"
Silhouette Coefficient: 0.015

tSNE
Silhouette Coefficient: 0.387
-------------------------------------------------------------------------------------------------------------------------------------------


-- 2021-01-09
Nuovo anno, decido di usare anche papers che sono presenti nel 2021 (magari).


-- 2020-12-15
Visto che ci sono dei paper che non hanno tutte le info in "metadata.csv", decido di rimuoverli dal dataset
e provare a fare un clustering usando solo quelli "perfetti", ovvero che hanno tutte le caratteristiche
che vogliamo (Cord_UID, Journal, Publish_Date, ...) partendo da Dataset201206_20_abstract_lem_eng_10.pickle.
vectorizer = TfidfVectorizer(stop_words='english', use_idf=True, 
                             smooth_idf=True, ngram_range=(1,2), min_df=100)
Uso anche bi-grams.
Traccio Elbow Curve: Report\Images\201206\Dataset201206_abstract_lem_eng_10_perfect.png
X = (28198, 2921)
Risultati (con KMeans++ K=5):
    Cluster 0: "viral" "protein" "cell" "immune" "infection" "antiviral" "vaccine" "human" "host" "drug"
    Cluster 1: "ace" "receptor" "spike" "protein" "binding" "expression" "entry" "enzyme" "spike protein" "enzyme ace"
    Cluster 2: "severe" "respiratory" "clinical" "infection" "day" "acute" "mortality" "risk" "syndrome" "pneumonia"
    Cluster 3: "model" "data" "paper" "analysis" "learning" "time" "based" "number" "method" "approach"
    Cluster 4: "health" "pandemic" "care" "social" "public" "mental" "risk" "medical" "anxiety" "psychological"
Silhouette Coefficient: 0.006316188024764012
Elbow curve dopo SVD: Report\Images\201206\Dataset201206_abstract_lem_eng_10_perfect.png
Risultati (SVD 20 components, KMeans++ K=5):
    Cluster 0: "pandemic" "health" "social" "care" "public" "people" "impact" "risk" "time" "medical"
    Cluster 1: "clinical" "day" "risk" "mortality" "infection" "positive" "patient" "treatment" "data" "age"
    Cluster 2: "viral" "protein" "cell" "infection" "immune" "human" "host" "antiviral" "binding" "ace"
    Cluster 3: "model" "data" "learning" "paper" "method" "analysis" "based" "approach" "time" "performance"
    Cluster 4: "respiratory" "severe" "acute" "syndrome" "acute respiratory" "respiratory syndrome" "severe acute" "infection" "clinical" "pandemic"
Silhouette Coefficient: 0.124

-- 2020-12-14
Esporto Dataset classificato usando questi parametri:
vectorizer = TfidfVectorizer(stop_words='english', use_idf=True, 
                             smooth_idf=True, ngram_range=(1,2), min_df=100) ## Corpus is in English
SVD a 20 e...
Cluster 0: "model" "epidemic" "number" "transmission" "data" "spread" "infection" "rate" "outbreak" "time"
Cluster 1: "viral" "protein" "cell" "infection" "immune" "human" "host" "antiviral" "ace" "binding"
Cluster 2: "clinical" "severe" "respiratory" "day" "risk" "infection" "mortality" "acute" "treatment" "patient"
Cluster 3: "health" "pandemic" "care" "social" "public" "medical" "risk" "impact" "people" "mental"
Cluster 4: "data" "model" "learning" "analysis" "paper" "method" "based" "approach" "social" "time"
Salvo Dataset201214Classification.json dove al suo interno c'è il coding (cord_uid, classificarion).
NOTA BENE: Classification è più piccolo di 20k rispetto al padre!
Dataset201206_20_abstract_lem_eng_10.pickle = 52339
Dataset201214Classification.json            = 28200

-- 2020-12-13
Sfida di oggi: togliere i duplicati e aumentare la len a 10 e vedere cosa succede.
APPLICO LEMMATIZER.
Prendo 52547 papers con len 10, e ne tolgo 208 perché duplicati. Totale: 52339.
Dataset201206_abstract_lem_eng_10.pickle => 52339 x 27482 words.
Prima di fare TfIdf per 5000, decido di graficare Elbow Curve con TfIdf a 100 (per fare prima).
Risultati (con KMeans++ K=5, max_features=5000):
    Cluster 0: data analysis pandemic time learning paper based detection clinical method
    Cluster 1: model epidemic number transmission data spread rate population outbreak infected
    Cluster 2: protein ace viral cell infection binding immune human host spike
    Cluster 3: health pandemic care public social mental anxiety psychological medical risk
    Cluster 4: severe respiratory clinical mortality infection day risk acute syndrome treatment
Risultati (SVD 20 components, KMeans++ K=5, max_features=5000):
    Cluster 0: health pandemic care social public medical impact mental risk people
    Cluster 1: clinical severe respiratory day risk infection acute mortality treatment patient
    Cluster 2: model number epidemic transmission data spread rate outbreak pandemic infection
    Cluster 3: data model learning analysis paper based social approach method time
    Cluster 4: viral protein cell infection immune human host ace antiviral vaccine

-- 2020-12-12
Dato il dataset di Cristina, lo inserisco nel mio notebook. 55120 x 146383 parole.
TfIdf a 5000 e computo i cluster in un'ora... Sempre troppo.
Risultati (con KMeans++ K=5, max_features=5000):
    Cluster 0: data based covid research study using results used model paper
    Cluster 1: patients covid disease clinical severe risk mortality sars respiratory infection
    Cluster 2: sars viral virus coronavirus infection ace2 protein covid respiratory cells
    Cluster 3: health covid pandemic care social healthcare public risk patients medical
    Cluster 4: cases model covid epidemic number countries transmission data spread population
Risultati (SVD 20 components, KMeans++ K=5, max_features=5000):
    Cluster 0: model data learning based research using models used analysis students
    Cluster 1: patients covid clinical disease severe risk respiratory treatment mortality care
    Cluster 2: sars viral virus cells protein cell infection covid human coronavirus
    Cluster 3: cases covid model number epidemic data transmission countries spread measures
    Cluster 4: health covid pandemic care social public healthcare mental research risk
Disegno anche SVD a 5 clusters su SVD a 20 components.

Prendo Stem_eng. 53839 x 20229 parole.
Prendo Lem_eng. 53821 x 27517 parole.

NUOVA SCOPERTA: Nel associare chiave-classificazione, ho scoperto che ci sono dei documenti che hanno
STESSO CONTENUTO --> Decido di eliminarli e di fare nei nuovi dataset. Non sono molti, circa 300, ma voglio
togliere altro noise.

-- 2020-12-10
Impossibile tracciare elbow curve per Dataset201206_20_abstract_CA.pickle.
Troppo grande.

-- 2020-12-09
Tracciato e salvato Elbow Curve per Dataset201206_20_abstract_stem_eng.pickle con TfIdf
max_features uguale a 5000.
Report/Images/201206/Dataset201206_20_abstract_stem_max500.png.

-- 2020-12-08
Dopo aver provato CA e Stem, costruisco Lem -> Dataset201206_20_abstract_lem_eng.pickle
Lem papers: 53821. 8 in meno rispetto a Stem. Anche qui, più di 5 parole nell'abstract.
I paper che non hanno passato il controllo li troviamo qui (Report/Discarded_papers_201206_lem.txt).
Risultati (con KMeans++ K=5, max_features=5000):
    Cluster 0: protein ace viral cell infection binding immune human host spike
    Cluster 1: data pandemic analysis time learning paper clinical based review detection
    Cluster 2: severe respiratory clinical infection mortality day acute risk syndrome treatment
    Cluster 3: health care pandemic public social mental anxiety psychological risk medical
    Cluster 4: model epidemic number transmission data spread rate population outbreak infected
Risultati (SVD 20 components, KMeans++ K=5, max_features=5000):
    Cluster 0: health pandemic care social public medical risk impact mental people
    Cluster 1: data learning model analysis social paper based pandemic approach method
    Cluster 2: viral protein cell infection immune human host antiviral ace binding
    Cluster 3: clinical severe respiratory day infection mortality risk acute treatment syndrome
    Cluster 4: model epidemic number transmission spread data social pandemic infection day

Nel frattempo analizzo Stem Dataset201206_20_abstract_stem_eng.pickle e max_features a 5000, 
applico anche qui SVD e t-SNE. 
Con SVD a 400 componenti traccio la varianza spiegata: 
Report/Images/201206/Cluster-SVD/Stem/Dataset201206_20_abstarct_stem_max5000_variance.png
L'Elbow curve: Report/Images/201206/Cluster-SVD/Stem/Dataset201206_20_abstarct_stem_SVD.png
E trovo i Cluster: Report/Images/201206/Cluster-SVD/Stem.
Silhouette score: 0.014.
Con t-SNE parto SVD a 400 componenti e ottengo un elbow curve 
Report/Images/201206/Cluster-tSNE/Stem/Dataset201206_20_abstarct_stem_t-SNE.png
E i seguenti cluster: Report/Images/201206/Cluster-tSNE/Stem.
Silhouette Coefficient: -0.026.

Continuo l'avventura con Lem analizzo Stem Dataset201206_20_abstract_lem_eng.pickle, max_features a 5000, 
applico SVD e t-SNE.
Con SVD a 400 componenti traccio la varianza spiegata: 
Report/Images/201206/Cluster-SVD/Lem/Dataset201206_20_abstarct_lem_max5000_variance.png
L'Elbow curve: Report/Images/201206/Cluster-SVD/Lem/Dataset201206_20_abstarct_lem_SVD.png
E trovo i Cluster: Report/Images/201206/Cluster-SVD/Lem.
Silhouette score: 0.012
t-SNE a 400 componenti.
Elbow curve: Report/Images/201206/Cluster-tSNE/Lem/Dataset201206_20_abstarct_lem_t-SNE.png
Cluster: Report/Images/201206/Cluster-tSNE/Lem e grafico Report/Images/201206/Cluster-tSNE/Lem/t-SNE.png
Silhouette score: -0.032


IMPORTANTE: in questi giorni ho notato che quando traccio l'elbow curve, diminuendo il numero di
componenti è meno visibile il fenomeno di "spike" ed esce una curva più "regolare", anche se
un vero e proprio elbow non si vede.
Ad oggi, direi che ci sono 5 macro cluster.

-- 2020-12-07
Nuovo incontro, nuove missioni:
- t-SNE, Cluster (metodo grafico)
- Silhouette score (metodo analitico)
- IDEA: Uso Covid, Coronavirus ecc... per prendere paper che trattano di questo.
    + lunghezza paper = 10, max_features = 50
--- DA IMPLEMENTARE GRAFICAMENTE I CLUSTER NEL SITO.

Dataset nuovo: 2020-12-06 con 374892 papers (35660 in più rispetto a 2020-11-06).
Estrazione con STEM: 53839 papers del 2020, non vuoti, con almeno 5 parole all'interno 
(pochine? Passerò a 10 se serve). Nel dataset precedente ne avevo 46962.
I paper che non hanno passato il controllo li troviamo qui (Report/Discarded_papers_201206_stem.txt).
Numero di parole all'interno: 20229. Proviamo con "max_features=50".
Dataset201206_20_abstract_stem_eng.pickle
Elbow curve: Report/Images/201206/Dataset201206_20_abstarct_stem_eng_max50.png
Risultati (con KMeans++ K=5, max_features=50):
    Cluster 0: model data number base time effect infect spread analysi diseas
    Cluster 1: viral protein activ infect immun human potenti respons effect develop
    Cluster 2: data test clinic time analysi posit risk day effect base
    Cluster 3: health pandem care social public impact global diseas effect time
    Cluster 4: sever respiratori diseas acut infect clinic risk treatment pandem patient


Nel frattempo studio l'algoritmo di Cristina. Mi tira fuori 55120 papers MA NON ELIMINA I PAPER
NON INGLESI('jk25uv4m' spagnolo, 'xutv2x8p' francese, '5m6sqwq8' empty, 'u5ol3nmm' spanish, ...).
Inoltre, non elimina paper con una sola parola o più che possono non avere senso ('wuqekxxc', 'h3lvbldz', ...)
In più vengono considerati anche i numeri, cosa che io non faccio, quindi nel momento in cui vengono
generate le parole io non ne tengo conto di queste, quindi il motivo per cui ho meno parole e meno
documenti è perché il numero di parole sarà superiore a 5 ed inoltre non fa stemming.
Ho notato un'altra feature di Cristina: prende solo parole che hanno almeno 4 caratteri.
Può avere senso? Numero di parole intanto: 146383.
Attuo TDIDF con "max_features=5000". Procedo con SVD vs t-SNE.
Il primo produce il seguente grafico Report/Images/Dataset201206_20_abstarct_stem_eng_max50.png
Applico SVD con n_components=400 (inizialmente era 10, ma su consiglio di Andrea cambio in 400, il
risultato è nella 400) 
e cerco 7 cluster ed ottengo: Report/Images/201206/Cluster-SVD/CA.
Silhouette score: 0.016
Il secondo con SVD a 400 components, t-SNE computato in 1 ora e 12min circa con 55120 documenti.
Trovo i seguenti cluster: Report/Images/201206/Cluster-tSNE/CA.
Silhouette score: -0.02/-0.03
----------------------------------------------------------------------------------------

-- 2020-12-01
Dopo molti giorni spesi nel buildare il dataset con BERT... NON SONO IN GRADO DI USARLO!
Upload Elbow Curve di Embs e CA.

-- 2020-11-20
Primo tentativo di usare BERT fallito, troppa RAM.
Nel frattempo provato algoritmo di Cristina ma.... risultati non incoraggianti

-- 2020-11-19
New dataset Dataset201106_20_abstract_stem_201119_eng.pickle: 46962, 19494 -- Stem. 30% ≃ 5848
Risultati (con KMeans++ K=5, max_features=5848):
    Cluster 0: protein viral ace immun infect cell bind vaccin express human
    Cluster 1: test detect posit sensit assay diagnost infect specif negat serolog
    Cluster 2: sever diseas respiratori clinic infect mortal risk acut syndrom day
    Cluster 3: health pandem care data social public time work effect medic
    Cluster 4: model epidem number transmiss spread infect data outbreak popul rate
Risultati (SVD dims=768, KMeans++ K=5):
    Cluster 0: test detect posit sensit assay diagnost infect specif viral negat
    Cluster 1: health pandem care data social public time work impact medic
    Cluster 2: sever diseas clinic respiratori infect mortal acut risk treatment day
    Cluster 3: model epidem number transmiss spread infect data time rate outbreak
    Cluster 4: protein viral ace immun cell infect bind express vaccin human

New dataset Dataset201106_20_abstract_lem_201119_eng.pickle: 46962, 26527 -- Lem. 30% ≃ 7958
Risultati (con KMeans++ K=5):
    Cluster 0: health care pandemic public social mental anxiety risk medical psychological
    Cluster 1: model epidemic number transmission spread data rate population outbreak disease
    Cluster 2: protein ace viral cell binding infection host human spike immune
    Cluster 3: severe respiratory disease clinical infection mortality acute day risk syndrome
    Cluster 4: data analysis pandemic time paper learning based detection clinical method
Risultati (SVD dims=768, KMeans++ K=5):
    Cluster 0: protein viral ace cell infection binding human immune host expression
    Cluster 1: health pandemic care public social medical risk mental disease impact
    Cluster 2: severe respiratory disease clinical infection acute day mortality risk treatment
    Cluster 3: data analysis paper learning time method detection based approach work
    Cluster 4: model epidemic number transmission data spread rate population outbreak time

Elbow curve tracciato per stem e lem.

-- 2020-11-18
Sulla base dei risultati ottenuti il giorno precedente (raccolti la maggior parte in "Strange_papers.txt" e "No_eng_papers.txt") ho deciso di
rimuovere alcune words che NLTK indica come inglese quando inglesi non sono, come:
- 'wir', 'das', 'sind', 'wird' 'ist' 'nach', 'word', 'count', 'se', 'las', 'di', 'bora', 'ana', 'silva', 'ant'
e in oltre se come risultato del parser ottengo meno di 5 parole, allora scarto il documento perché potrebbe essere il risultato di un documento
ipoteticamente tedesco che ha come stemma alcune delle parole in inglese.
Nuovo dataset: 46964 papers (in Discarded_papers.txt si possono vedere quali non sono stati presi). Ma ancora c'è qualche elemento all'interno
dei paper che non viene scartato, come 'um', 'ist'.... (controllare meglio stopwords). Siamo passati da 76689 papers a 46964 dove almeno 1000
sono stati rimossi perché corti o non avendo contenuti. Ma la cosa "strana" è che siamo passati da 137292 parole a 19497 (30% ≃ 5850).
Traccio Elbow Curve per Dataset201106_20_201118_eng_len5 (NB: CI SONO GLI ABSTRACT) con max_features = 5850.
(Report\Images\Dataset201106_20_abstract_201118_eng_len5_max5820.png)
Risultati (con KMeans++ K=5):
    Cluster 0: health pandem care data social public time work effect impact
    Cluster 1: test detect posit sensit assay diagnost infect specif negat serolog
    Cluster 2: sever diseas respiratori clinic infect mortal risk acut syndrom day
    Cluster 3: model epidem number transmiss spread infect data outbreak popul rate
    Cluster 4: protein viral ace immun infect cell bind vaccin express human
Risultati (SVD dims=768, KMeans++ K=5):
    Cluster 0: model epidem number transmiss spread data infect time rate outbreak
    Cluster 1: sever diseas clinic respiratori infect mortal acut risk treatment day
    Cluster 2: test detect posit sensit assay diagnost infect specif negat viral
    Cluster 3: health pandem care social data public time work impact effect
    Cluster 4: protein viral ace immun cell infect bind express vaccin human

-- 2020-11-14/17
Elbow curve costruito solo su "Dataset201106_20_abstract_lem" con il max_features=48162. (Report\Images\Dataset201106_20_abstract_lem_max48162.png)
Testato file "cord_19_embeddings_2020-11-06.csv" tracciando anche qui l'Elbow Curve. Risultato ideale.
Quindi ho provato ad usare BERT sui documenti con stemming ma senza successo: troppo tempo.
Allora ho deciso di guardare meglio i documenti che avevo a disposizione: path doppi, dove alle volte alcuni non contenevano 'abstract' e quindi
restituivano testi vuoti. Soluzione: analizzo tutti i path che ha a disposizione e se ha l'abstract allora lo prendo.
Ancora, alcuni testi sono francesi, tedeschi, spagnoli.. Decido di rimuovere tutti questi documenti usando 'words' di NLTK in modo tale 
che se non sono nel dizionario allora al 99% è un documento "non inglese".
Quindi sono passato da 76689 documenti a circa 48000, tutto ciò per applicare BERT e ottimizzare i tempi.
Inoltre, sono state aggiunte nuove stopwords da me (anche usando un file).
Risultati (SVD da 768 a dims=200, KMeans++ K=5):
    Cluster 0: test detect posit sensit diagnost assay infect specif viral negat
    Cluster 1: protein viral cell immun infect ace express activ bind human
    Cluster 2: health pandem care social public impact medic effect work time
    Cluster 3: model data number epidem paper time base learn method predict
    Cluster 4: sever diseas clinic respiratori infect risk acut mortal treatment day

-- 2020-11-13
Elbow curve costruito solo su "Dataset201106_20_abstract_lem" (Report\Images\Dataset201106_20_abstract_lem.png)

-- 2020-11-09
Dataset costruiti (Dataset[anno/mese/giorno]_[annopaper]_[abstract|text]_[stemming|lemmatizer]): 
    - Dataset201106_20_text_lem; --> Paper: 76689, Words: 1066496
Elbow curve costruito solo su "Dataset201106_20_abstract_stem" con il max_features=41188 (Report\Images\Dataset201106_20_abstract_stem_max41188.png)
Elbow curve costruito solo su "Dataset201106_20_abstract_stem" (Report\Images\Dataset201106_20_abstract_stem.png)


-- 2020-11-08
Dataset costruiti (Dataset[anno/mese/giorno]_[annopaper]_[abstract|text]_[stemming|lemmatizer]): 
    - Dataset201106_20_abstract_stem; --> Paper: 76689, Words: 137292
    - Dataset201106_20_abstract_lem; --> Paper: 76689, Words: 160539
    - Dataset201106_20_text_stem. --> Paper: 76689, Words: 944694

Elbow curve costruito solo su "Dataset201106_20_abstract_stem" con il max_features=50 (Report\Images\Dataset201106_20_abstract_stem_max50.png)
Risultati (con KMeans K=5):
    Cluster 0: sarscov infect virus coronavirus viral protein cell covid respiratori sever
    Cluster 1: use model studi data result effect develop method differ provid
    Cluster 2: cell diseas present protein activ provid respons number care develop
    Cluster 3: patient covid clinic hospit diseas sever care studi treatment infect
    Cluster 4: covid pandem health diseas case care studi risk use coronavirus
Risultati (SVD dims=20, KMeans K=5):
    Cluster 0: cell diseas test activ respons clinic protein increas develop care
    Cluster 1: patient covid clinic hospit diseas care sever studi treatment infect
    Cluster 2: covid pandem health diseas case studi care risk use coronavirus
    Cluster 3: use model studi data result develop effect differ method time
    Cluster 4: sarscov infect virus coronavirus cell protein viral covid respiratori sever

Elbow curve costruito solo su "Dataset201106_20_abstract_lem" con il max_features=50 (Report\Images\Dataset201106_20_abstract_lem_max50.png)
Risultati (con KMeans K=5):
    Cluster 0: sarscov infect virus coronavirus viral protein cell covid respiratori sever
    Cluster 1: cell diseas present protein activ provid respons number care develop
    Cluster 2: covid pandem health diseas case care studi risk use coronavirus
    Cluster 3: use model studi data result effect develop method differ provid
    Cluster 4: patient covid clinic hospit diseas sever care studi treatment infect
Risultati (SVD dims=20, KMeans K=5):
    Cluster 0: use model studi data result develop effect differ method time
    Cluster 1: covid pandem health diseas case studi care risk use coronavirus
    Cluster 2: cell test diseas activ clinic respons develop care increas provid
    Cluster 3: sarscov infect virus coronavirus cell protein viral covid respiratori sever
    Cluster 4: patient covid clinic hospit diseas care sever studi treatment infect